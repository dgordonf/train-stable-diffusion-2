{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c679eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-16T19:17:11.083648Z",
     "iopub.status.busy": "2022-10-16T19:17:11.082584Z",
     "iopub.status.idle": "2022-10-16T19:17:29.027805Z",
     "shell.execute_reply": "2022-10-16T19:17:29.026994Z",
     "shell.execute_reply.started": "2022-10-16T19:17:11.083569Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade diffusers transformers scipy ftfy\n",
    "!pip install flax==0.5.0 --no-deps\n",
    "!pip install ipywidgets msgpack rich "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db43f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-16T19:17:29.029312Z",
     "iopub.status.busy": "2022-10-16T19:17:29.029066Z",
     "iopub.status.idle": "2022-10-16T19:17:30.872252Z",
     "shell.execute_reply": "2022-10-16T19:17:30.871135Z",
     "shell.execute_reply.started": "2022-10-16T19:17:29.029285Z"
    }
   },
   "outputs": [],
   "source": [
    "#Your need a Hugging Face account and token for this to work. Info on on how to get your token here: https://huggingface.co/docs/hub/security-tokens\n",
    "#Once you have your token, replace 'YOUR_HF_TOKEN_GOES_HERE' below with your token\n",
    "\n",
    "!wget https://raw.githubusercontent.com/gradient-ai/stable-diffusion/main/login.py\n",
    "!python login.py --token YOUR_HF_TOKEN_GOES_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582f73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-16T19:17:30.874089Z",
     "iopub.status.busy": "2022-10-16T19:17:30.873560Z",
     "iopub.status.idle": "2022-10-16T19:17:31.471369Z",
     "shell.execute_reply": "2022-10-16T19:17:31.470468Z",
     "shell.execute_reply.started": "2022-10-16T19:17:30.874060Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c528f2",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In order to generate an image, you simply need to run one of the two cells below. The first cell is optimized for low power GPUs, like the Free GPU M4000, and will be able to generate an image on any GPU powered Gradient Machine. \n",
    "\n",
    "The next cell, is optimized to run on more powerful GPU setups, like an A100 or A6000, and can be used to quickly generate high quality images on these machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ced90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-16T19:17:41.034293Z",
     "iopub.status.busy": "2022-10-16T19:17:41.033932Z",
     "iopub.status.idle": "2022-10-16T19:20:34.816637Z",
     "shell.execute_reply": "2022-10-16T19:20:34.815586Z",
     "shell.execute_reply.started": "2022-10-16T19:17:41.034263Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "\n",
    "model_id = \"your-account/model-v1\"\n",
    "device = \"cuda\"\n",
    "\n",
    "# Use the Euler scheduler here instead\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 3\n",
    "lst = []\n",
    "prompt = '''A photo of sks'''\n",
    "\n",
    "for i in range(sample_num):\n",
    "    with autocast(\"cuda\"):\n",
    "        a = pipe(prompt, guidance_scale=7.5, height=512, width=512,\n",
    "                 num_inference_steps=100, seed=343434, scheduler='LMSDiscreteScheduler')[\"images\"][0]\n",
    "        lst.append(a)\n",
    "        display(a)\n",
    "        a.save(f'outputs/gen-image-{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8fe2f68fb170d3e59715732971b3e459b293954a8edfc2b4e8212f6d304449f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
